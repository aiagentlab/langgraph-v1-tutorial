{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì—ì´ì „íŠ¸(Agent)\n",
    "\n",
    "ì—ì´ì „íŠ¸ëŠ” ì–¸ì–´ ëª¨ë¸ê³¼ ë„êµ¬ë¥¼ ê²°í•©í•˜ì—¬ ì‘ì—…ì— ëŒ€í•´ ì¶”ë¡ í•˜ê³ , ì‚¬ìš©í•  ë„êµ¬ë¥¼ ê²°ì •í•˜ë©°, ì†”ë£¨ì…˜ì„ í–¥í•´ ë°˜ë³µì ìœ¼ë¡œ ì‘ì—…í•  ìˆ˜ ìˆëŠ” ì‹œìŠ¤í…œì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "\n",
    "`create_agent`ëŠ” í”„ë¡œë•ì…˜ ìˆ˜ì¤€ì˜ ì—ì´ì „íŠ¸ êµ¬í˜„ì„ ì œê³µí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith ì¶”ì ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "[í”„ë¡œì íŠ¸ëª…]\n",
      "LangChain-V1-Tutorial\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv(override=True)\n",
    "# ì¶”ì ì„ ìœ„í•œ í”„ë¡œì íŠ¸ ì´ë¦„ ì„¤ì •\n",
    "logging.langsmith(\"LangChain-V1-Tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ëª¨ë¸ (Model)\n",
    "\n",
    "ì—ì´ì „íŠ¸ì˜ ì¶”ë¡  ì—”ì§„ì¸ LLM ì€ ê°„ë‹¨í•˜ê²Œ `provider:model` í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# ëª¨ë¸ ì‹ë³„ì ë¬¸ìì—´ì„ ì‚¬ìš©í•œ ê°„ë‹¨í•œ ë°©ë²•\n",
    "agent = create_agent(\"openai:gpt-4.1-mini\", tools=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•˜ì§€ë§Œ, ëª¨ë¸ì˜ ì„¸ë¶€ ì„¤ì •ì„ ìœ„í•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ ë‹¤ì–‘í•œ ì˜µì…˜ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì§ì ‘ ì´ˆê¸°í™”í•˜ì—¬ ë” ì„¸ë°€í•œ ì œì–´\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    temperature=0.1,  # ì‘ë‹µì˜ ë¬´ì‘ìœ„ì„± ì œì–´\n",
    "    max_tokens=1000,  # ìµœëŒ€ ìƒì„± í† í° ìˆ˜\n",
    "    timeout=30,  # ìš”ì²­ íƒ€ì„ì•„ì›ƒ(ì´ˆ)\n",
    ")\n",
    "\n",
    "agent = create_agent(model, tools=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë™ì  ëª¨ë¸\n",
    "\n",
    "ë™ì  ëª¨ë¸ì€ ëŸ°íƒ€ì„ì— í˜„ì¬ ìƒíƒœì™€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„ íƒë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì •êµí•œ ë¼ìš°íŒ… ë¡œì§ê³¼ ë¹„ìš© ìµœì í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "![](assets/wrap_model_call.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ModelRequest`ëŠ” agentì˜ ëª¨ë¸ í˜¸ì¶œ ì •ë³´ë¥¼ ë‹´ëŠ” dataclassë¡œ, middlewareì—ì„œ ìš”ì²­ì„ ê²€ì‚¬í•˜ê³  ìˆ˜ì •í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
    "\n",
    "# ê¸°ë³¸ ëª¨ë¸ê³¼ ê³ ê¸‰ ëª¨ë¸ ì •ì˜\n",
    "basic_model = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "advanced_model = ChatOpenAI(model=\"gpt-4.1\")\n",
    "\n",
    "\n",
    "@wrap_model_call\n",
    "def dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n",
    "    \"\"\"ëŒ€í™” ë³µì¡ë„ì— ë”°ë¼ ëª¨ë¸ ì„ íƒ\"\"\"\n",
    "    message_count = len(request.state[\"messages\"])\n",
    "\n",
    "    # ê¸´ ëŒ€í™”ì—ëŠ” ê³ ê¸‰ ëª¨ë¸ ì‚¬ìš©\n",
    "    if message_count > 10:\n",
    "        model = advanced_model\n",
    "    else:\n",
    "        model = basic_model\n",
    "\n",
    "    request.model = model\n",
    "    return handler(request)\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=basic_model, tools=[], middleware=[dynamic_model_selection]  # ê¸°ë³¸ ëª¨ë¸\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mmodel\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ë¬¼ë¡ ì…ë‹ˆë‹¤! ë¨¸ì‹ ëŸ¬ë‹(Machine Learning)ì˜ ë™ì‘ ì›ë¦¬ì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "### ë¨¸ì‹ ëŸ¬ë‹ì˜ ë™ì‘ ì›ë¦¬\n",
      "\n",
      "ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì»´í“¨í„°ê°€ ìŠ¤ìŠ¤ë¡œ ê·œì¹™ì´ë‚˜ íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ì˜ˆì¸¡ì´ë‚˜ íŒë‹¨ì„ í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ì „í†µì ì¸ í”„ë¡œê·¸ë˜ë°ì´ ì‚¬ëŒì´ ì§ì ‘ ê·œì¹™ì„ ì½”ë”©í•˜ëŠ” ê²ƒê³¼ ë‹¬ë¦¬, ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¥¼ í†µí•´ ê·œì¹™ì„ ìë™ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.\n",
      "\n",
      "1. **ë°ì´í„° ìˆ˜ì§‘**  \n",
      "   ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë‹¤ì–‘í•œ ì–‘ì§ˆì˜ ë°ì´í„°ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì‚¬ì§„, í…ìŠ¤íŠ¸, ìˆ«ì ë“± ì—¬ëŸ¬ í˜•íƒœê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "2. **ë°ì´í„° ì „ì²˜ë¦¬**  \n",
      "   ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ì •ì œ(cleaning), ì •ê·œí™”(normalization), íŠ¹ì§• ì¶”ì¶œ(feature extraction) ë“±ì˜ ê³¼ì •ì„ ê±°ì³ ëª¨ë¸ í•™ìŠµì— ì í•©í•œ í˜•íƒœë¡œ ë§Œë“­ë‹ˆë‹¤.\n",
      "\n",
      "3. **ëª¨ë¸ ì„ íƒ**  \n",
      "   ë¬¸ì œì˜ ì„±ê²©ì— ë§ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜(ì˜ˆ: ì„ í˜• íšŒê·€, ì˜ì‚¬ê²°ì •íŠ¸ë¦¬, ì‹ ê²½ë§ ë“±)ì„ ì„ íƒí•©ë‹ˆë‹¤.\n",
      "\n",
      "4. **í•™ìŠµ(Training)**  \n",
      "   ì¤€ë¹„ëœ ë°ì´í„°ë¥¼ ëª¨ë¸ì— ì…ë ¥í•´ ëª¨ë¸ì˜ ë‚´ë¶€ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤. ëª¨ë¸ì´ ë°ì´í„°ì˜ íŒ¨í„´ì„ ì¸ì‹í•˜ë„ë¡ í•˜ê¸° ìœ„í•´ ì†ì‹¤ í•¨ìˆ˜(loss function)ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ìµœì í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
      "\n",
      "5. **í‰ê°€(Evaluation)**  \n",
      "   í•™ìŠµëœ ëª¨ë¸ì„ ìƒˆë¡œìš´ ë°ì´í„°(í…ŒìŠ¤íŠ¸ ë°ì´í„°)ì— ëŒ€í•´ í‰ê°€í•˜ì—¬ ì–¼ë§ˆë‚˜ ì˜ ì˜ˆì¸¡í•˜ëŠ”ì§€ ì„±ëŠ¥ì„ í™•ì¸í•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨, F1 ì ìˆ˜ ë“± ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
      "\n",
      "6. **ì˜ˆì¸¡ ë° ì ìš©**  \n",
      "   ì„±ëŠ¥ì´ ê²€ì¦ëœ ëª¨ë¸ì„ ì‹¤ì œ í™˜ê²½ì— ì ìš©í•´ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡í•˜ê±°ë‚˜ ë¶„ë¥˜ ë“±ì˜ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "### ìš”ì•½í•˜ìë©´:  \n",
      "ë¨¸ì‹ ëŸ¬ë‹ì€ **ë°ì´í„° â†’ í•™ìŠµ(íŒ¨í„´ ì¸ì‹) â†’ ì˜ˆì¸¡**ì˜ ê³¼ì •ì„ í†µí•´ ë™ì‘í•˜ë©°, ëª¨ë¸ì´ ìŠ¤ìŠ¤ë¡œ ê²½í—˜(ë°ì´í„°)ì„ í†µí•´ ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” íŠ¹ì§•ì„ ê°–ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "í•„ìš”í•˜ë©´ ì¢€ ë” êµ¬ì²´ì ì¸ ì•Œê³ ë¦¬ì¦˜ì´ë‚˜ ì‚¬ë¡€ì— ëŒ€í•´ì„œë„ ì„¤ëª…í•´ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.messages import stream_graph\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "stream_graph(\n",
    "    agent,\n",
    "    inputs={\n",
    "        \"messages\": [HumanMessage(content=\"ë¨¸ì‹ ëŸ¬ë‹ì˜ ë™ì‘ ì›ë¦¬ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì¤˜\")]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìˆ˜ì •ì— í™œìš©í•  **ì£¼ìš” ì†ì„±**ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "* `model`: ì‚¬ìš©í•  `BaseChatModel` ì¸ìŠ¤í„´ìŠ¤\n",
    "* `system_prompt`: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (optional)\n",
    "* `messages`: ëŒ€í™” ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì œì™¸)\n",
    "* `tool_choice`: tool ì„ íƒ ì„¤ì •\n",
    "* `tools`: ì‚¬ìš© ê°€ëŠ¥í•œ tool ë¦¬ìŠ¤íŠ¸\n",
    "* `response_format`: ì‘ë‹µ í˜•ì‹ ì§€ì •\n",
    "* `state`: í˜„ì¬ agent ìƒíƒœ (`AgentState`)\n",
    "* `runtime`: agent runtime ì •ë³´\n",
    "* `model_settings`: ì¶”ê°€ ëª¨ë¸ ì„¤ì • (dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@wrap_model_call\n",
    "def dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n",
    "    \"\"\"ëŒ€í™” ë³µì¡ë„ì— ë”°ë¼ ëª¨ë¸ ì„ íƒ\"\"\"\n",
    "    message_count = len(request.state[\"messages\"][-1].content)\n",
    "    print(f\"ê¸€ììˆ˜: {message_count}\")\n",
    "\n",
    "    # ê¸´ ëŒ€í™”ì—ëŠ” ê³ ê¸‰ ëª¨ë¸ ì‚¬ìš©\n",
    "    if message_count > 10:\n",
    "        # ì—¬ëŸ¬ ì†ì„± ë™ì‹œ ë³€ê²½\n",
    "        new_request = request.override(\n",
    "            model=advanced_model,\n",
    "            system_prompt=\"emoji ë¥¼ ì‚¬ìš©í•´ì„œ ë‹µë³€í•´ì¤˜\",\n",
    "            tool_choice=\"auto\",\n",
    "        )\n",
    "        return handler(new_request)\n",
    "    else:\n",
    "        new_request = request.override(\n",
    "            system_prompt=\"í•œ ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì¤˜. emoji ëŠ” ì‚¬ìš©í•˜ì§€ ë§ì•„ì¤˜.\",\n",
    "            tool_choice=\"auto\",\n",
    "            model=basic_model,\n",
    "        )\n",
    "        return handler(new_request)\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=basic_model, tools=[], middleware=[dynamic_model_selection]  # ê¸°ë³¸ ëª¨ë¸\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê¸€ììˆ˜ 10ì ë¯¸ë§Œì¼ ë•Œì˜ ì‘ë‹µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê¸€ììˆ˜: 9\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mmodel\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ í•™ìŠµí•´ ì˜ˆì¸¡ì´ë‚˜ ë¶„ë¥˜ ëª¨ë¸ì„ ë§Œë“œëŠ” ê³¼ì •ì…ë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "stream_graph(agent, inputs={\"messages\": [HumanMessage(content=\"ë¨¸ì‹ ëŸ¬ë‹ ë™ì‘ì›ë¦¬\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê¸€ììˆ˜ 10ì ì´ìƒì¼ ë•Œì˜ ì‘ë‹µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê¸€ììˆ˜: 25\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mmodel\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ë¬¼ë¡ ì´ì£ ! ë¨¸ì‹ ëŸ¬ë‹ì˜ ë™ì‘ ì›ë¦¬ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•´ë“œë¦´ê²Œìš” ğŸ˜Š\n",
      "\n",
      "1. ë°ì´í„° ìˆ˜ì§‘ ğŸ“Š  \n",
      "ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì€ ë¨¼ì € í•™ìŠµì— ì‚¬ìš©í•  ë°ì´í„°ë¥¼ ë§ì´ ëª¨ì•„ìš”.\n",
      "\n",
      "2. ë°ì´í„° ì „ì²˜ë¦¬ ğŸ§¹  \n",
      "ëª¨ì€ ë°ì´í„°ë¥¼ ê¹¨ë—í•˜ê²Œ ì •ë¦¬í•˜ê³ , í•„ìš”í•œ í˜•íƒœë¡œ ë°”ê¿”ìš”.\n",
      "\n",
      "3. ëª¨ë¸ ì„ ì • ë° í•™ìŠµ ğŸ§   \n",
      "ì ì ˆí•œ ì•Œê³ ë¦¬ì¦˜(ì˜ˆ: ì„ í˜•íšŒê·€, ê²°ì •íŠ¸ë¦¬ ë“±)ì„ ì„ íƒí•˜ê³ , ë°ì´í„°ë¥¼ ì´ìš©í•´ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œìš”. ì¦‰, ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ ì°¾ë„ë¡ í•´ìš”.\n",
      "\n",
      "4. ì˜ˆì¸¡ ë° í‰ê°€ ğŸ”  \n",
      "í•™ìŠµëœ ëª¨ë¸ì„ ì´ìš©í•´ì„œ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ì„ í•´ë³´ê³ , ì‹¤ì œ ê°’ê³¼ ë¹„êµí•˜ë©° ì–¼ë§ˆë‚˜ ì˜ ë§ì·„ëŠ”ì§€ í‰ê°€í•´ìš”.\n",
      "\n",
      "5. ëª¨ë¸ ê°œì„  ğŸ”„  \n",
      "í‰ê°€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ì„ ë‹¤ì‹œ í•™ìŠµì‹œí‚¤ê±°ë‚˜, ë°ì´í„°ë¥¼ ë” ë³´ê°•í•´ì„œ ì„±ëŠ¥ì„ ë†’ì—¬ìš”.\n",
      "\n",
      "ì´ë ‡ê²Œ ë°˜ë³µí•˜ë©° ì ì  ë” ë˜‘ë˜‘í•œ ëª¨ë¸ì„ ë§Œë“œëŠ” ê²Œ ë¨¸ì‹ ëŸ¬ë‹ì˜ ê¸°ë³¸ì ì¸ ë™ì‘ ì›ë¦¬ì˜ˆìš”! ğŸš€"
     ]
    }
   ],
   "source": [
    "stream_graph(\n",
    "    agent,\n",
    "    inputs={\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"ë¨¸ì‹ ëŸ¬ë‹ì˜ ë™ì‘ ì›ë¦¬ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\")\n",
    "        ]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë„êµ¬ (Tools)\n",
    "\n",
    "ë„êµ¬ëŠ” ì—ì´ì „íŠ¸ì—ê²Œ í–‰ë™ì„ ì·¨í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ë¶€ì—¬í•©ë‹ˆë‹¤. \n",
    "\n",
    "ì—ì´ì „íŠ¸ëŠ” ë‹¤ìŒì„ ì§€ì›í•©ë‹ˆë‹¤.\n",
    "- ìˆœì°¨ì ìœ¼ë¡œ ì—¬ëŸ¬ ë„êµ¬ í˜¸ì¶œ\n",
    "- ì ì ˆí•œ ê²½ìš° ë³‘ë ¬ ë„êµ¬ í˜¸ì¶œ\n",
    "- ì´ì „ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë™ì  ë„êµ¬ ì„ íƒ\n",
    "- ë„êµ¬ ì¬ì‹œë„ ë¡œì§ ë° ì˜¤ë¥˜ ì²˜ë¦¬\n",
    "- ë„êµ¬ í˜¸ì¶œ ê°„ ìƒíƒœ ì§€ì†ì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë„êµ¬ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for information.\"\"\"\n",
    "    return f\"Results for: {query}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get weather information for a location.\"\"\"\n",
    "    # ì¼ë¶€ëŸ¬ ì˜¤ë¥˜ ë°œìƒ\n",
    "    raise Exception(\"ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "# ë„êµ¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ì—ì´ì „íŠ¸ì— ì „ë‹¬\n",
    "agent = create_agent(\"openai:gpt-4.1-mini\", tools=[search, get_weather])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë„êµ¬ ì˜¤ë¥˜ ì²˜ë¦¬\n",
    "\n",
    "ë„êµ¬ ì˜¤ë¥˜ ì²˜ë¦¬ë¥¼ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•˜ë ¤ë©´ `@wrap_tool_call` ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¯¸ë“¤ì›¨ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import wrap_tool_call\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "@wrap_tool_call\n",
    "def handle_tool_errors(request, handler):\n",
    "    \"\"\"ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜ë¥¼ ì»¤ìŠ¤í…€ ë©”ì‹œì§€ë¡œ ì²˜ë¦¬\"\"\"\n",
    "    try:\n",
    "        return handler(request)\n",
    "    except Exception as e:\n",
    "        # ëª¨ë¸ì— ì»¤ìŠ¤í…€ ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜\n",
    "        return ToolMessage(\n",
    "            content=f\"[ì—ëŸ¬] ë„êµ¬ í˜¸ì¶œì‹œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. ({str(e)})\",\n",
    "            tool_call_id=request.tool_call[\"id\"],\n",
    "        )\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4.1-mini\",\n",
    "    tools=[search, get_weather],\n",
    "    middleware=[handle_tool_errors],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mmodel\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mtools\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "[ì—ëŸ¬] ë„êµ¬ í˜¸ì¶œì‹œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. (ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤.)\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mmodel\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì£„ì†¡í•˜ì§€ë§Œ, í˜„ì¬ ì„œìš¸ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì‹œê² ì–´ìš”?"
     ]
    }
   ],
   "source": [
    "stream_graph(agent, inputs={\"messages\": [HumanMessage(content=\"ì„œìš¸ ë‚ ì”¨ ì¡°íšŒí•´ì¤˜\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í”„ë¡¬í”„íŠ¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\n",
    "\n",
    "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì œê³µí•˜ì—¬ ì—ì´ì „íŠ¸ê°€ ì‘ì—…ì— ì ‘ê·¼í•˜ëŠ” ë°©ì‹ì„ í˜•ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    \"openai:gpt-4.1-mini\",\n",
    "    tools=[search, get_weather],\n",
    "    system_prompt=\"You are a helpful assistant. Be concise and accurate.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë™ì  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\n",
    "\n",
    "ëŸ°íƒ€ì„ ì»¨í…ìŠ¤íŠ¸ë‚˜ ì—ì´ì „íŠ¸ ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•´ì•¼ í•˜ëŠ” ê³ ê¸‰ ì‚¬ìš© ì‚¬ë¡€ì˜ ê²½ìš° ë¯¸ë“¤ì›¨ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- í•µì‹¬: `request.runtime.context`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "\n",
    "class Context(TypedDict):\n",
    "    answer_type: str\n",
    "\n",
    "\n",
    "@dynamic_prompt\n",
    "def user_role_prompt(request: ModelRequest) -> str:\n",
    "    \"\"\"ì‚¬ìš©ì ì—­í• ì— ë”°ë¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±\"\"\"\n",
    "    answer_type = request.runtime.context.get(\"answer_type\", \"default\")\n",
    "    print(answer_type)\n",
    "    base_prompt = \"You are a helpful assistant.\"\n",
    "\n",
    "    if answer_type == \"default\":\n",
    "        return f\"{base_prompt} Answer in Korean. ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì¤˜.\"\n",
    "    elif answer_type == \"sns\":\n",
    "        return f\"{base_prompt} Answer in Korean. SNS í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì¤˜.\"\n",
    "    elif answer_type == \"article\":\n",
    "        return f\"{base_prompt} Answer in Korean. ë‰´ìŠ¤ ê¸°ì‚¬ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì¤˜.\"\n",
    "\n",
    "    return base_prompt\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4.1-mini\",\n",
    "    tools=[search],\n",
    "    middleware=[user_role_prompt],\n",
    "    context_schema=Context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mmodel\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "[ë‰´ìŠ¤ ê¸°ì‚¬ í˜•ì‹]\n",
      "\n",
      "ë¨¸ì‹ ëŸ¬ë‹ì˜ ë™ì‘ ì›ë¦¬, ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜?\n",
      "\n",
      "ìµœê·¼ ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì˜ í•µì‹¬ìœ¼ë¡œ ì£¼ëª©ë°›ëŠ” ë¨¸ì‹ ëŸ¬ë‹(Machine Learning)ì€ ë°ì´í„°ë¥¼ í†µí•´ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ê³  ì˜ˆì¸¡ ë˜ëŠ” íŒë‹¨ì„ ìˆ˜í–‰í•˜ëŠ” ê¸°ìˆ ì´ë‹¤. ë¨¸ì‹ ëŸ¬ë‹ì˜ ë™ì‘ ì›ë¦¬ëŠ” í¬ê²Œ ë°ì´í„° ìˆ˜ì§‘, ì „ì²˜ë¦¬, í•™ìŠµ, í‰ê°€, ê·¸ë¦¬ê³  ì˜ˆì¸¡ì˜ ê³¼ì •ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.\n",
      "\n",
      "ë¨¼ì €, ë¨¸ì‹ ëŸ¬ë‹ì€ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì„ ìˆ˜ì§‘í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•œë‹¤. ì´ ë°ì´í„°ëŠ” ëª¨ë¸ì´ í•™ìŠµí•  ê¸°ì´ˆ ìë£Œê°€ ëœë‹¤. ì´í›„ ë°ì´í„°ì˜ í’ˆì§ˆì„ ë†’ì´ê¸° ìœ„í•´ ê²°ì¸¡ì¹˜ ì²˜ë¦¬, ì •ê·œí™” ë“±ì˜ ì „ì²˜ë¦¬ ì‘ì—…ì´ ì´ë£¨ì–´ì§„ë‹¤.\n",
      "\n",
      "ë‹¤ìŒ ë‹¨ê³„ëŠ” ëª¨ë¸ í•™ìŠµì´ë‹¤. ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì€ ì£¼ì–´ì§„ ë°ì´í„°ì—ì„œ íŒ¨í„´ì´ë‚˜ ê·œì¹™ì„ ì°¾ì•„ ë‚´ë¶€ ìˆ˜í•™ì  ëª¨ë¸ì„ êµ¬ì¶•í•œë‹¤. ëŒ€í‘œì ì¸ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œëŠ” íšŒê·€ë¶„ì„, ì˜ì‚¬ê²°ì •ë‚˜ë¬´, ì‹ ê²½ë§ ë“±ì´ ìˆë‹¤. í•™ìŠµ ê³¼ì •ì—ì„œ ëª¨ë¸ì€ ì…ë ¥ ë°ì´í„°ì™€ ì •ë‹µ ë°ì´í„°(ë¼ë²¨)ë¥¼ í†µí•´ ì˜ˆì¸¡ ëŠ¥ë ¥ì„ ì ì§„ì ìœ¼ë¡œ ë†’ì¸ë‹¤.\n",
      "\n",
      "í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸ì€ í‰ê°€ë¥¼ ê±°ì³ ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨ ë“±ì˜ ì„±ëŠ¥ ì§€í‘œë¥¼ í™•ì¸í•œë‹¤. í‰ê°€ ê²°ê³¼ê°€ ë§Œì¡±ìŠ¤ëŸ¬ìš¸ ê²½ìš°, ì‹¤ì œ ë°ì´í„°ì— ì ìš©í•´ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ê±°ë‚˜ ë¶„ë¥˜í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤.\n",
      "\n",
      "ì´ì²˜ëŸ¼ ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤ìŠ¤ë¡œ íŒ¨í„´ì„ ë°œê²¬í•˜ê³ , ì´ë¥¼ í†µí•´ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ë¡œ ë‹¤ì–‘í•œ ì‚°ì—… ë¶„ì•¼ì—ì„œ í˜ì‹ ì„ ì´ëŒê³  ìˆë‹¤.\n",
      "\n",
      "ì „ë¬¸ê°€ë“¤ì€ \"ë¨¸ì‹ ëŸ¬ë‹ì€ ë‹¨ìˆœí•œ ê¸°ìˆ ì„ ë„˜ì–´ ë°ì´í„° ì´í•´ì™€ ì²˜ë¦¬ ëŠ¥ë ¥ì˜ ì§‘ì•½ì²´\"ë¼ë©°, \"ì ì ˆí•œ ë°ì´í„° í™•ë³´ì™€ ì•Œê³ ë¦¬ì¦˜ ì„ íƒì´ ì„±ëŠ¥ í–¥ìƒì˜ í•µì‹¬\"ì´ë¼ê³  ê°•ì¡°í–ˆë‹¤."
     ]
    }
   ],
   "source": [
    "# ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ë™ì ìœ¼ë¡œ ì„¤ì •ë©ë‹ˆë‹¤\n",
    "stream_graph(\n",
    "    agent,\n",
    "    inputs={\n",
    "        \"messages\": [HumanMessage(content=\"ë¨¸ì‹ ëŸ¬ë‹ì˜ ë™ì‘ ì›ë¦¬ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì¤˜\")]\n",
    "    },\n",
    "    context=Context(answer_type=\"article\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì—ì´ì „íŠ¸ í˜¸ì¶œ\n",
    "\n",
    "ì—ì´ì „íŠ¸ì˜ ìƒíƒœì— ì—…ë°ì´íŠ¸ë¥¼ ì „ë‹¬í•˜ì—¬ ì—ì´ì „íŠ¸ë¥¼ í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë“  ì—ì´ì „íŠ¸ëŠ” ìƒíƒœì— ë©”ì‹œì§€ ì‹œí€€ìŠ¤ë¥¼ í¬í•¨í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mmodel\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ í•™ìŠµí•´ ì˜ˆì¸¡ì´ë‚˜ ë¶„ë¥˜ ë“±ì˜ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ê¸°ë³¸ ì›ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. ë°ì´í„° ìˆ˜ì§‘: í•™ìŠµì— í•„ìš”í•œ ë°ì´í„°ë¥¼ ëª¨ìë‹ˆë‹¤.\n",
      "2. ë°ì´í„° ì „ì²˜ë¦¬: ê²°ì¸¡ì¹˜ ì²˜ë¦¬, ì •ê·œí™” ë“± ë°ì´í„°ë¥¼ í•™ìŠµì— ì í•©í•˜ê²Œ ê°€ê³µí•©ë‹ˆë‹¤.\n",
      "3. ëª¨ë¸ ì„ íƒ: ë¬¸ì œ ìœ í˜•ì— ë§ëŠ” ì•Œê³ ë¦¬ì¦˜(ì˜ˆ: íšŒê·€, ë¶„ë¥˜, êµ°ì§‘ ë“±)ì„ ì„ íƒí•©ë‹ˆë‹¤.\n",
      "4. í•™ìŠµ: ëª¨ë¸ì´ ì£¼ì–´ì§„ ë°ì´í„°ë¡œë¶€í„° ê·œì¹™ì´ë‚˜ íŒ¨í„´ì„ ì°¾ì•„ë‚´ë„ë¡ í›ˆë ¨í•©ë‹ˆë‹¤.\n",
      "5. í‰ê°€: í•™ìŠµí•œ ëª¨ë¸ì„ ìƒˆë¡œìš´ ë°ì´í„°ì— ì ìš©í•´ ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.\n",
      "6. ì˜ˆì¸¡ ë° ì‘ìš©: í•™ìŠµëœ ëª¨ë¸ë¡œ ì‹¤ì œ ë¬¸ì œë¥¼ ì˜ˆì¸¡í•˜ê±°ë‚˜ ë¶„ë¥˜í•©ë‹ˆë‹¤.\n",
      "\n",
      "ì¦‰, ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¡œë¶€í„° ê·œì¹™ì„ ìë™ìœ¼ë¡œ ë°œê²¬í•´ ë¬¸ì œ í•´ê²°ì— í™œìš©í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "stream_graph(\n",
    "    agent,\n",
    "    inputs={\n",
    "        \"messages\": [HumanMessage(content=\"ë¨¸ì‹ ëŸ¬ë‹ì˜ ë™ì‘ ì›ë¦¬ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì¤˜\")]\n",
    "    },\n",
    "    context=Context(answer_type=\"default\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê³ ê¸‰ ê°œë…\n",
    "\n",
    "### êµ¬ì¡°í™”ëœ ì¶œë ¥\n",
    "\n",
    "íŠ¹ì • í˜•ì‹ìœ¼ë¡œ ì—ì´ì „íŠ¸ì˜ ì¶œë ¥ì„ ë°˜í™˜í•˜ê³  ì‹¶ì„ ë•Œê°€ ìˆìŠµë‹ˆë‹¤. LangChainì€ `response_format` ë§¤ê°œë³€ìˆ˜ë¥¼ í†µí•´ êµ¬ì¡°í™”ëœ ì¶œë ¥ ì „ëµì„ ì œê³µí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ToolStrategy\n",
    "\n",
    "`ToolStrategy`ëŠ” ì¸ê³µì ì¸ ë„êµ¬ í˜¸ì¶œì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤. ë„êµ¬ í˜¸ì¶œì„ ì§€ì›í•˜ëŠ” ëª¨ë“  ëª¨ë¸ì—ì„œ ì‘ë™í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='John Doe' email='john@example.com' phone='(555) 123-4567'\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "\n",
    "\n",
    "# ì‘ë‹µ ìŠ¤í‚¤ë§ˆ ì •ì˜\n",
    "class ContactInfo(BaseModel):\n",
    "    name: str\n",
    "    email: str\n",
    "    phone: str\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4.1-mini\", tools=[], response_format=ToolStrategy(ContactInfo)\n",
    ")\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(result[\"structured_response\"])\n",
    "# ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ProviderStrategy\n",
    "\n",
    "`ProviderStrategy`ëŠ” ëª¨ë¸ ì œê³µìì˜ ë„¤ì´í‹°ë¸Œ êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„±ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë” ì•ˆì •ì ì´ì§€ë§Œ ë„¤ì´í‹°ë¸Œ êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ì§€ì›í•˜ëŠ” ì œê³µì(ì˜ˆ: OpenAI)ì—ì„œë§Œ ì‘ë™í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.structured_output import ProviderStrategy\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4.1\", response_format=ProviderStrategy(ContactInfo)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"structured_response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë©”ëª¨ë¦¬\n",
    "\n",
    "ì—ì´ì „íŠ¸ëŠ” ë©”ì‹œì§€ ìƒíƒœë¥¼ í†µí•´ ëŒ€í™” ê¸°ë¡ì„ ìë™ìœ¼ë¡œ ìœ ì§€í•©ë‹ˆë‹¤. ëŒ€í™” ì¤‘ì— ì¶”ê°€ ì •ë³´ë¥¼ ê¸°ì–µí•˜ê¸° ìœ„í•´ ì»¤ìŠ¤í…€ ìƒíƒœ ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì—ì´ì „íŠ¸ë¥¼ êµ¬ì„±í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ìƒíƒœì— ì €ì¥ëœ ì •ë³´ëŠ” ì—ì´ì „íŠ¸ì˜ ë‹¨ê¸° ë©”ëª¨ë¦¬ë¡œ ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë¯¸ë“¤ì›¨ì–´ë¥¼ í†µí•œ ìƒíƒœ ì •ì˜\n",
    "\n",
    "ì»¤ìŠ¤í…€ ìƒíƒœê°€ íŠ¹ì • ë¯¸ë“¤ì›¨ì–´ í›… ë° í•´ë‹¹ ë¯¸ë“¤ì›¨ì–´ì— ì—°ê²°ëœ ë„êµ¬ì—ì„œ ì•¡ì„¸ìŠ¤í•´ì•¼ í•˜ëŠ” ê²½ìš° ë¯¸ë“¤ì›¨ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì»¤ìŠ¤í…€ ìƒíƒœë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langchain.agents import AgentState\n",
    "from langchain.agents.middleware import AgentMiddleware\n",
    "\n",
    "\n",
    "# ì»¤ìŠ¤í…€ ìƒíƒœ ìŠ¤í‚¤ë§ˆ ì •ì˜\n",
    "class CustomState(AgentState):\n",
    "    user_preferences: dict\n",
    "\n",
    "\n",
    "class CustomMiddleware(AgentMiddleware):\n",
    "    state_schema = CustomState\n",
    "    tools = []\n",
    "\n",
    "    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n",
    "        # ëª¨ë¸ í˜¸ì¶œ ì „ ì»¤ìŠ¤í…€ ë¡œì§\n",
    "        pass\n",
    "\n",
    "\n",
    "agent = create_agent(\"openai:gpt-4.1-mini\", tools=[], middleware=[CustomMiddleware()])\n",
    "\n",
    "# ì—ì´ì „íŠ¸ëŠ” ì´ì œ ë©”ì‹œì§€ ì™¸ì— ì¶”ê°€ ìƒíƒœë¥¼ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\n",
    "        \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `state_schema`ë¥¼ í†µí•œ ìƒíƒœ ì •ì˜\n",
    "\n",
    "ë„êµ¬ì—ì„œë§Œ ì‚¬ìš©ë˜ëŠ” ì»¤ìŠ¤í…€ ìƒíƒœë¥¼ ì •ì˜í•˜ëŠ” ë‹¨ì¶• ë°©ë²•ìœ¼ë¡œ `state_schema` ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentState\n",
    "\n",
    "\n",
    "class CustomState(AgentState):\n",
    "    user_preferences: dict\n",
    "\n",
    "\n",
    "agent = create_agent(\"openai:gpt-4.1-mini\", tools=[], state_schema=CustomState)\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\n",
    "        \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìŠ¤íŠ¸ë¦¬ë°\n",
    "\n",
    "ì—ì´ì „íŠ¸ê°€ ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ëŠ” ê²½ìš° ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¤‘ê°„ ì§„í–‰ ìƒí™©ì„ í‘œì‹œí•˜ê¸° ìœ„í•´ ë©”ì‹œì§€ê°€ ë°œìƒí•˜ëŠ” ëŒ€ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in agent.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Search for AI news and summarize the findings\"}\n",
    "        ]\n",
    "    },\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    # ê° ì²­í¬ì—ëŠ” í•´ë‹¹ ì‹œì ì˜ ì „ì²´ ìƒíƒœê°€ í¬í•¨ë©ë‹ˆë‹¤\n",
    "    latest_message = chunk[\"messages\"][-1]\n",
    "    if latest_message.content:\n",
    "        print(f\"Agent: {latest_message.content}\")\n",
    "    elif hasattr(latest_message, \"tool_calls\") and latest_message.tool_calls:\n",
    "        print(f\"Calling tools: {[tc['name'] for tc in latest_message.tool_calls]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë¯¸ë“¤ì›¨ì–´\n",
    "\n",
    "ë¯¸ë“¤ì›¨ì–´ëŠ” ì‹¤í–‰ì˜ ë‹¤ì–‘í•œ ë‹¨ê³„ì—ì„œ ì—ì´ì „íŠ¸ ë™ì‘ì„ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•˜ê¸° ìœ„í•œ ê°•ë ¥í•œ í™•ì¥ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "**ë¯¸ë“¤ì›¨ì–´ ì‚¬ìš© ì‚¬ë¡€:**\n",
    "\n",
    "- ëª¨ë¸ í˜¸ì¶œ ì „ ìƒíƒœ ì²˜ë¦¬ (ì˜ˆ: ë©”ì‹œì§€ íŠ¸ë¦¬ë°, ì»¨í…ìŠ¤íŠ¸ ì£¼ì…)\n",
    "- ëª¨ë¸ ì‘ë‹µ ìˆ˜ì • ë˜ëŠ” ê²€ì¦ (ì˜ˆ: ê°€ë“œë ˆì¼, ì½˜í…ì¸  í•„í„°ë§)\n",
    "- ì»¤ìŠ¤í…€ ë¡œì§ìœ¼ë¡œ ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜ ì²˜ë¦¬\n",
    "- ìƒíƒœ ë˜ëŠ” ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¥¸ ë™ì  ëª¨ë¸ ì„ íƒ\n",
    "- ì»¤ìŠ¤í…€ ë¡œê¹…, ëª¨ë‹ˆí„°ë§ ë˜ëŠ” ë¶„ì„ ì¶”ê°€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatOpenAI í´ë˜ìŠ¤\n",
    "\n",
    "OpenAIì˜ chat model APIë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì„¤ì¹˜ ë° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U langchain-openai\n",
    "# ë˜ëŠ”\n",
    "# uv add langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í™˜ê²½ ë³€ìˆ˜ ì„¤ì •:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export OPENAI_API_KEY=\"your-api-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì£¼ìš” ì´ˆê¸°í™” íŒŒë¼ë¯¸í„°\n",
    "\n",
    "#### Completion íŒŒë¼ë¯¸í„°\n",
    "\n",
    "| íŒŒë¼ë¯¸í„° | íƒ€ì… | ì„¤ëª… |\n",
    "|:---|:---|:---|\n",
    "| `model` | `str` | ì‚¬ìš©í•  OpenAI ëª¨ë¸ ì´ë¦„ |\n",
    "| `temperature` | `float` | ìƒ˜í”Œë§ ì˜¨ë„ |\n",
    "| `max_tokens` | `int \\| None` | ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ |\n",
    "| `logprobs` | `bool \\| None` | logprobs ë°˜í™˜ ì—¬ë¶€ |\n",
    "| `stream_options` | `dict` | ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ ì„¤ì • (ì˜ˆ: `{\"include_usage\": True}`) |\n",
    "| `use_responses_api` | `bool \\| None` | Responses API ì‚¬ìš© ì—¬ë¶€ |\n",
    "\n",
    "#### Client íŒŒë¼ë¯¸í„°\n",
    "\n",
    "| íŒŒë¼ë¯¸í„° | íƒ€ì… | ì„¤ëª… |\n",
    "|:---|:---|:---|\n",
    "| `timeout` | `float \\| Tuple[float, float] \\| Any \\| None` | ìš”ì²­ íƒ€ì„ì•„ì›ƒ |\n",
    "| `max_retries` | `int \\| None` | ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ |\n",
    "| `api_key` | `str \\| None` | OpenAI API í‚¤ (ë¯¸ì§€ì •ì‹œ `OPENAI_API_KEY` í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©) |\n",
    "| `base_url` | `str \\| None` | API ìš”ì²­ base URL (proxy ë˜ëŠ” emulator ì‚¬ìš©ì‹œ) |\n",
    "| `organization` | `str \\| None` | OpenAI organization ID (ë¯¸ì§€ì •ì‹œ `OPENAI_ORG_ID` í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê¸°ë³¸ ì‚¬ìš©ë²•\n",
    "\n",
    "#### ëª¨ë¸ ì´ˆê¸°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë©”ì‹œì§€ í˜¸ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "response = model.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‘ë‹µ í˜•ì‹:\n",
    "```python\n",
    "AIMessage(\n",
    "    content=\"J'adore la programmation.\",\n",
    "    response_metadata={\n",
    "        \"token_usage\": {\"completion_tokens\": 5, \"prompt_tokens\": 31, \"total_tokens\": 36},\n",
    "        \"model_name\": \"gpt-4o\",\n",
    "        \"finish_reason\": \"stop\",\n",
    "    },\n",
    "    usage_metadata={\"input_tokens\": 31, \"output_tokens\": 5, \"total_tokens\": 36},\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìŠ¤íŠ¸ë¦¬ë°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in model.stream(messages):\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì „ì²´ ë©”ì‹œì§€ ìˆ˜ì§‘:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = model.stream(messages)\n",
    "full = next(stream)\n",
    "for chunk in stream:\n",
    "    full += chunk\n",
    "print(full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë¹„ë™ê¸° ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¨ì¼ í˜¸ì¶œ\n",
    "# await model.ainvoke(messages)\n",
    "\n",
    "# ìŠ¤íŠ¸ë¦¬ë°\n",
    "# async for chunk in model.astream(messages):\n",
    "#     print(chunk.content, end=\"\")\n",
    "\n",
    "# ë°°ì¹˜ ì²˜ë¦¬\n",
    "# await model.abatch([messages])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class GetWeather(BaseModel):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "\n",
    "    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
    "\n",
    "\n",
    "model_with_tools = model.bind_tools([GetWeather])\n",
    "response = model_with_tools.invoke(\"What's the weather in LA?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë³‘ë ¬ tool call ë¹„í™œì„±í™”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model_with_tools.invoke(\n",
    "    \"What is the weather in LA and NY?\", parallel_tool_calls=False\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    rating: int | None = Field(description=\"How funny the joke is, from 1 to 10\")\n",
    "\n",
    "\n",
    "structured_model = model.with_structured_output(Joke)\n",
    "result = structured_model.invoke(\"Tell me a joke about cats\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë©”ì„œë“œ ì˜µì…˜\n",
    "\n",
    "* `json_schema`: OpenAI Structured Output API ì‚¬ìš© (ê¸°ë³¸ê°’)\n",
    "* `function_calling`: OpenAI tool-calling API ì‚¬ìš©\n",
    "* `json_mode`: OpenAI JSON mode ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_model = model.with_structured_output(\n",
    "    schema=Joke, method=\"json_schema\", include_raw=True, strict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_model = model.bind(response_format={\"type\": \"json_object\"})\n",
    "response = json_model.invoke(\n",
    "    \"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]\"\n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì´ë¯¸ì§€ ì…ë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import httpx\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
    "image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n",
    "\n",
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"describe the weather in this image\"},\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = model.invoke([message])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token ì‚¬ìš©ëŸ‰ ì¶”ì "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(messages)\n",
    "print(response.usage_metadata)\n",
    "# {\"input_tokens\": 28, \"output_tokens\": 5, \"total_tokens\": 33}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìŠ¤íŠ¸ë¦¬ë°ì‹œ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = model.stream(messages, stream_usage=True)\n",
    "full = next(stream)\n",
    "for chunk in stream:\n",
    "    full += chunk\n",
    "print(full.usage_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs_model = model.bind(logprobs=True)\n",
    "response = logprobs_model.invoke(messages)\n",
    "print(response.response_metadata[\"logprobs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI í˜¸í™˜ API ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LM Studio ì˜ˆì‹œ\n",
    "# model = ChatOpenAI(\n",
    "#     base_url=\"http://localhost:1234/v1\",\n",
    "#     api_key=\"lm-studio\",\n",
    "#     model=\"mlx-community/QwQ-32B-4bit\",\n",
    "#     extra_body={\"ttl\": 300}\n",
    "# )\n",
    "\n",
    "# vLLM ì˜ˆì‹œ\n",
    "# model = ChatOpenAI(\n",
    "#     base_url=\"http://localhost:8000/v1\",\n",
    "#     api_key=\"EMPTY\",\n",
    "#     model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "#     extra_body={\"use_beam_search\": True, \"best_of\": 4}\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### íŒŒë¼ë¯¸í„° êµ¬ë¶„\n",
    "\n",
    "**`model_kwargs` ì‚¬ìš©:**\n",
    "* í‘œì¤€ OpenAI API íŒŒë¼ë¯¸í„°\n",
    "* ìµœìƒìœ„ ìš”ì²­ payloadì— ë³‘í•©ë˜ëŠ” íŒŒë¼ë¯¸í„°\n",
    "\n",
    "**`extra_body` ì‚¬ìš©:**\n",
    "* OpenAI í˜¸í™˜ providerì˜ ì»¤ìŠ¤í…€ íŒŒë¼ë¯¸í„°\n",
    "* `extra_body` í‚¤ í•˜ìœ„ì— ì¤‘ì²©ë˜ëŠ” íŒŒë¼ë¯¸í„°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_kwargs ì˜ˆì‹œ\n",
    "# model = ChatOpenAI(\n",
    "#     model=\"gpt-4o\",\n",
    "#     model_kwargs={\n",
    "#         \"stream_options\": {\"include_usage\": True},\n",
    "#         \"max_completion_tokens\": 300,\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# extra_body ì˜ˆì‹œ\n",
    "# model = ChatOpenAI(\n",
    "#     base_url=\"http://localhost:8000/v1\",\n",
    "#     extra_body={\n",
    "#         \"use_beam_search\": True,\n",
    "#         \"best_of\": 4,\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Caching\n",
    "\n",
    "ë°˜ë³µì ì¸ promptê°€ ìˆëŠ” ëŒ€ìš©ëŸ‰ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ cache hit rate í–¥ìƒ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(messages, prompt_cache_key=\"example-key-a\")\n",
    "\n",
    "# ë™ì  cache key\n",
    "dynamic_suffix = \"user123\"\n",
    "cache_key = f\"example-key-{dynamic_suffix}\"\n",
    "response = model.invoke(messages, prompt_cache_key=cache_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
